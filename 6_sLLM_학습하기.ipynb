{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "mount_file_id": "19QctOfMRWzWsqLa42N5rOzPPuYGN8amS",
      "authorship_tag": "ABX9TyPLE1eozAUsE3iP7NnPv/EC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuJaeJeong/llm-study-repo/blob/master/6_sLLM_%ED%95%99%EC%8A%B5%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요 라이브러리 설치"
      ],
      "metadata": {
        "id": "dQmcRC0D9124"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "id": "jx5lSWUmFuip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eut0EDms7tUB"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torch torchvision torchaudio triton bitsandbytes transformers -y\n",
        "!pip install torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install triton==3.0.0\n",
        "!pip install bitsandbytes==0.44.1\n",
        "!pip install transformers==4.46.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub autotrain-advanced -qqq"
      ],
      "metadata": {
        "id": "JDDjNzvF_o0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQL 생성 프롬프트"
      ],
      "metadata": {
        "id": "AiV33srlDkWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(ddl, question, query=''):\n",
        "  prompt = f\"\"\"당신은 SQL을 생성하는 SQL봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
        "  ### DDL:\n",
        "  {ddl}\n",
        "\n",
        "  ### Question:\n",
        "  {question}\n",
        "\n",
        "  ### SQL:\n",
        "  {question}\"\"\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "dwt41ATpD327"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT 평가 프롬프트와 코드 준비"
      ],
      "metadata": {
        "id": "qdttm0qrJo02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "def make_requests_for_gpt_evaluation(df, filename, dir='requests'):\n",
        "  if not Path(dir).exists():\n",
        "    Path(dir).mkdir(parents=True)\n",
        "  prompts=[]\n",
        "  for idx, row in df.iterrows():\n",
        "    prompts.append(\"\"\"Based on below DDL and Question, evaluate gen_sql can resolve Question. If gen_sql and gt_sql do equal job, return \\\"yes\\\" else return \\\"no\\\". Output JSON Format: {\\\"resolve_yn\\\": \\\"\\\"}\"\"\" + f\"\"\"\n",
        "      DDL: {row['context']}\n",
        "      Question: {row['question']}\n",
        "      gen_sql: {row['gen_sql']}\n",
        "      gt_sql: {row['answer']}\"\"\"\n",
        "    )\n",
        "  jobs = [{\"model\": \"gpt-4o\", \"response_format\": {\"type\": \"json_object\"}, \"messages\": [{\"role\": \"system\", \"content\": prompt}]} for prompt in prompts]\n",
        "  with open(Path(dir, filename), \"w\", encoding=\"utf-8\") as f:\n",
        "    for job in jobs:\n",
        "      json_string = json.dumps(job, ensure_ascii=False)\n",
        "      f.write(json_string + '\\n')"
      ],
      "metadata": {
        "id": "e1eRFXGjEZNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기초 모델로 생성하기"
      ],
      "metadata": {
        "id": "c1fHzuM4QHtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive 마운트\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 원하는 저장 경로 설정 (Google Drive 내부 폴더)\n",
        "local_dir = \"/content/drive/MyDrive/models/Yi-Ko-6B\"\n",
        "\n",
        "# 모델 다운로드 및 저장\n",
        "snapshot_download(repo_id=\"beomi/Yi-Ko-6B\", local_dir=local_dir)"
      ],
      "metadata": {
        "id": "loCLpUES7frT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "def make_inference_pipeline(model_id):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "  # model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.bfloat16)\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "  )\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', quantization_config=quant_config)\n",
        "  pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "  return pipe\n",
        "\n",
        "model_id = \"/content/drive/MyDrive/models/Yi-Ko-6B\"\n",
        "hf_pipe = make_inference_pipeline(model_id)\n",
        "example = \"\"\"당신은 SQL을 생성하는 SQL봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
        "\n",
        "### DDL:\n",
        "CREATE TABLE users (\n",
        "  player_id INT PRIMARY KEY AUTO_INCREMENT,\n",
        "  username VARCHAR(255) UNIQUE NOT NULL,\n",
        "  email VARCHAR(255) UNIQUE NOT NULL\n",
        "  password_hash VARCHAR(255) NOT NULL,\n",
        "  date_joined DATETIME NOT NULL,\n",
        "  last_login DATETIME\n",
        ");\n",
        "\n",
        "### Question:\n",
        "사용자 이름에 'admin'이 포함되어 있는 계정의 수를 알려주세요\n",
        "\n",
        "### SQL:\n",
        "\"\"\"\n",
        "hf_pipe(example, do_sample=False, return_full_text=False, max_length=1024, truncation=True)"
      ],
      "metadata": {
        "id": "4Uh3gS0rJv9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기초 모델 성능 측정"
      ],
      "metadata": {
        "id": "VndxZZNGZ64e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RKskvGq6nzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 데이터셋 불러오기\n",
        "df = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")['test']\n",
        "df = df.to_pandas()\n",
        "for idx, row in df.iterrows():\n",
        "  prompt = make_prompt(row['context'], row['question'])\n",
        "  df.loc[idx, 'prompt'] = prompt\n",
        "\n",
        "# sql 생성\n",
        "gen_sqls = hf_pipe(df['prompt'].tolist(), do_sample=False, return_full_text=False, max_length=512, truncation=True)\n",
        "gen_sqls = [x[0]['generated_text'] for x in gen_sqls]\n",
        "df['gen_sql'] = gen_sqls\n",
        "\n",
        "# 평가를 위한 requests.jsonl 생성\n",
        "eval_filepath = \"text2sql_evaluation.jsonl\"\n",
        "make_requests_for_gpt_evaluation(df, eval_filepath)"
      ],
      "metadata": {
        "id": "6SUVjgRvQ_QD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4 평가수행\n",
        "!python api_request_parallel_processor.py \\\n",
        "  --requests_filepath requests/{eval_filepath} \\\n",
        "  --save_filepath requests/{eval_filepath} \\\n",
        "  --request_url https://api.openai.com/v1/chat/completions \\\n",
        "  --max_requests_per_minute 2500 \\\n",
        "  --token_encoding_name cl100k_base \\\n",
        "  --max_attempts 5 \\\n",
        "  --logging_level 20"
      ],
      "metadata": {
        "id": "-efrZ-EHEXib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CAG6St43Q_zX"
      }
    }
  ]
}